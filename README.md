# AgentIA - Python Code Standardizer

Application web pour analyser, corriger (PEP8) et documenter automatiquement du code Python.

![Stack](https://img.shields.io/badge/Stack-Python%20%2B%20FastAPI%20%2B%20JS-blue)
![License](https://img.shields.io/badge/License-MIT-green)

## FonctionnalitÃ©s

- ğŸ“ **Upload de dossiers** â€” Importez plusieurs fichiers Python ou un ZIP
- ğŸ” **Analyse statique** â€” DÃ©tection des erreurs PEP8 avec Flake8
- ğŸ› ï¸ **Correction automatique** â€” Correction PEP8 avec autopep8
- ğŸ¤– **Docstrings IA** â€” GÃ©nÃ©ration automatique via LLM (API ou Ollama)
- ğŸ“¦ **Export ZIP** â€” TÃ©lÃ©chargez vos fichiers corrigÃ©s

## Architecture

```
agentia_v2/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # API FastAPI
â”‚   â”œâ”€â”€ analyser.py         # Analyse statique (AST, Flake8)
â”‚   â”œâ”€â”€ corrector.py        # Correction PEP8
â”‚   â”œâ”€â”€ generator_docstring.py  # GÃ©nÃ©ration docstrings
â”‚   â”œâ”€â”€ llm_service.py      # Service LLM (API ou Ollama)
â”‚   â””â”€â”€ utils.py            # Utilitaires
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html          # Interface web
â”‚   â”œâ”€â”€ style.css           # Styles
â”‚   â””â”€â”€ app.js              # Logic JS
â”œâ”€â”€ uploads/                # Fichiers uploadÃ©s (temporaire)
â”œâ”€â”€ outputs/                # Fichiers traitÃ©s
â”œâ”€â”€ .env.example            # Configuration exemple
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Installation

### 1. Cloner et installer

```bash
# CrÃ©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### 2. Configurer le LLM

**Option A : Ollama (local, gratuit)**

```bash
# Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh  # Linux/Mac
# ou: winget install Ollama.Ollama  # Windows

# TÃ©lÃ©charger un modÃ¨le
ollama pull llama3.2:3b
```

**Option B : API externe (OpenAI, etc.)**

```bash
# Copier le fichier de configuration
cp .env.example .env

# Ã‰diter .env avec vos credentials
LLM_API_URL=https://api.openai.com/v1/chat/completions
LLM_API_TOKEN=sk-...
LLM_MODEL=gpt-4
```

### 3. Lancer l'application

```bash
cd backend
python app.py
```

Ouvrir http://localhost:8000

## API Endpoints

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/api/status` | Statut de l'API et backend LLM |
| POST | `/api/upload` | Upload fichiers Python/ZIP |
| GET | `/api/analyze/{job_id}` | Analyse des fichiers |
| POST | `/api/process/{job_id}` | Traitement (PEP8 + docstrings) |
| GET | `/api/download/{job_id}` | TÃ©lÃ©charger le ZIP rÃ©sultat |
| GET | `/api/preview/{job_id}/{file}` | PrÃ©visualiser un fichier |
| DELETE | `/api/job/{job_id}` | Supprimer un job |

## Utilisation

1. **Glissez-dÃ©posez** vos fichiers Python ou un ZIP
2. **Consultez l'analyse** â€” Score, fonctions, classes, erreurs
3. **Choisissez les options** â€” PEP8 (obligatoire) + Docstrings (optionnel)
4. **Lancez le traitement**
5. **TÃ©lÃ©chargez** vos fichiers corrigÃ©s

## Configuration LLM

L'application dÃ©tecte automatiquement la configuration :

- Si `LLM_API_URL` et `LLM_API_TOKEN` sont dÃ©finis â†’ utilise l'API
- Sinon â†’ utilise Ollama en local

Le modÃ¨le par dÃ©faut est `llama3.2:3b`, modifiable via `LLM_MODEL`.

## DÃ©veloppement

```bash
# Mode dÃ©veloppement avec rechargement automatique
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

## License

MIT
